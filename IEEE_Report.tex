\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% ==================== Packages (IEEE-friendly) ====================
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[caption=false,font=footnotesize]{subfig} % recommended for IEEEtran
\usepackage{siunitx}
\usepackage{fancyhdr}

% ==================== Page numbers (bottom-center) ====================
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% ==================== Helpers ====================
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Comparative Analysis of Self-Supervised Learning Approaches for Prostate Cancer Histopathology Grading\\
{\footnotesize CP8321 Deep Learning - Final Project}
}

\author{
\IEEEauthorblockN{Parsa Ranjbaran}
\IEEEauthorblockA{\textit{Department of Computer Science}\\
\textit{Toronto Metropolitan University}\\
Toronto, Canada\\
Student ID: 501037874}
\and
\IEEEauthorblockN{Abdur Qadeer}
\IEEEauthorblockA{\textit{Department of Computer Science}\\
\textit{Toronto Metropolitan University}\\
Toronto, Canada\\
Student ID: 500967819}
\and
\IEEEauthorblockN{Satvik Kaul}
\IEEEauthorblockA{\textit{Department of Computer Science}\\
\textit{Toronto Metropolitan University}\\
Toronto, Canada\\
Student ID: 501312329}
}

\maketitle
\thispagestyle{fancy} % ensure page number shows on first page as well

\begin{abstract}
Self-supervised learning (SSL) is increasingly used in medical imaging to learn transferable features when labels are limited or expensive. This work presents a controlled comparison of three learning strategies for prostate histopathology grading on the SICAPv2 dataset: (1) supervised learning from random initialization (baseline), (2) reconstruction-based SSL using a convolutional autoencoder (CAE), and (3) contrastive SSL using SimCLR with NT-Xent loss. All methods share the same encoder backbone to isolate the effect of representation learning. We evaluate downstream performance on four Gleason categories (Non-Cancerous, Grade 3, Grade 4, Grade 5) and report accuracy, macro and weighted F1, Cohen's $\kappa$, confusion matrices, and ROC curves. Reconstruction-based SSL substantially improves over the baseline (62.8\% vs.\ 47.6\% accuracy; $\kappa$ 0.500 vs.\ 0.341), while SimCLR underperforms in our compute-constrained setting (37.6\% accuracy; $\kappa$ 0.067) and collapses minority-grade predictions. We provide a detailed analysis explaining why contrastive SSL can fail in histopathology when batch size and augmentation regimes are not tuned to the domain, and we document actionable remedies for future work.
\end{abstract}

\begin{IEEEkeywords}
self-supervised learning, contrastive learning, histopathology, prostate cancer, Gleason grading, medical imaging, SimCLR, autoencoder
\end{IEEEkeywords}

% =========================================================
\section{Introduction}
% =========================================================
Prostate cancer is among the most prevalent malignancies in men, and histopathology-based Gleason grading is a clinical standard for assessing tumor aggressiveness and guiding treatment decisions. Manual grading is time-consuming and can vary across pathologists, motivating automated approaches \cite{gleason2002,nagpal2019}. Deep learning models have achieved strong results on pathology classification tasks, but traditional supervised training requires large labeled datasets and can generalize poorly under stain variation and dataset shift.

Self-supervised learning (SSL) provides a practical alternative by learning visual representations from unlabeled data using pretext objectives, then transferring those representations to downstream classification. In natural image settings, contrastive SSL methods such as SimCLR \cite{chen2020simclr} and MoCo \cite{he2020moco} often match or exceed supervised pretraining. However, histopathology introduces domain-specific challenges: subtle morphological differences between grades, high stain variability, and severe class imbalance. Additionally, many contrastive objectives assume large batch sizes (or many negatives), which may be infeasible under GPU memory constraints.

\subsection{Project Goal and Scope}
This project evaluates SSL for patch-level Gleason grading using SICAPv2. We compare:
\begin{itemize}
    \item \textbf{Baseline (Supervised from Scratch):} End-to-end supervised training with random initialization.
    \item \textbf{Reconstruction-based SSL (CAE):} Unsupervised pretraining via image reconstruction followed by classification training.
    \item \textbf{Contrastive SSL (SimCLR):} Pretraining with augmented-view contrastive learning followed by classification training.
\end{itemize}
All approaches use the same encoder backbone to ensure a fair comparison.

\subsection{Contributions}
\begin{enumerate}
    \item A controlled, apples-to-apples comparison of supervised, reconstruction-based SSL, and contrastive SSL on SICAPv2.
    \item Empirical evidence that CAE-style SSL can significantly improve downstream grading performance in this setting.
    \item A detailed failure analysis of SimCLR under compute constraints, grounded in observed metrics and plots.
    \item A reproducible codebase and reporting artifacts, including consolidated comparison plots, training curves, confusion matrices, and ROC curves.
\end{enumerate}

% =========================================================
\section{Background and Related Work}
% =========================================================
\subsection{Self-Supervised Learning in Vision}
SSL defines a supervised-like objective using automatically generated pseudo-labels. Two dominant paradigms are:
\begin{itemize}
    \item \textbf{Reconstruction-based SSL:} Learn features by reconstructing inputs (often with an encoder-decoder). This can emphasize local texture and structure.
    \item \textbf{Contrastive SSL:} Learn features by pulling together two augmented views of the same sample and pushing apart representations from different samples. SimCLR \cite{chen2020simclr} popularized a simple framework using strong augmentations and large-batch training.
\end{itemize}

\subsection{SSL in Digital Pathology}
Histopathology has unique properties: staining differences can be larger than inter-class differences; discriminative evidence is often localized; and classes frequently form an ordinal continuum (e.g., Gleason grades). Prior work demonstrated that SSL trained directly on pathology data can outperform ImageNet transfer learning \cite{ciga2020}. Domain shift analyses emphasize that augmentation policies should be tailored to staining and tissue morphology \cite{stacke2021}. Standard stain normalization approaches (e.g., Macenko) are frequently used to reduce stain variability, though we do not apply explicit stain normalization in this project \cite{macenko2009}.

\subsection{Prostate Cancer Grading}
SICAP provides a curated set of prostate tissue patches with Gleason annotations \cite{silva2019sicap}. In patch classification, minor grades are often confused with dominant classes, and Grade 5 can be difficult to learn due to both limited examples and morphological overlap with Grade 4.

% =========================================================
\section{Dataset and Preprocessing}
% =========================================================
\subsection{Dataset}
We use SICAPv2 \cite{silva2019sicap}, composed of approximately 18,000 RGB patches of size $128 \times 128$. We follow the provided train/test split.

\subsection{Class Definitions and Imbalance}
We model four categories: NC, G3, G4, and G5. The dataset is imbalanced. Table~\ref{tab:classdist} summarizes the approximate class distribution (percentages).

\begin{table}[t]
\centering
\caption{Approximate Class Distribution (SICAPv2)}
\label{tab:classdist}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Class} & \textbf{Meaning} & \textbf{Approx.\ Share} \\
\midrule
NC & Non-cancerous & 30.4\% \\
G3 & Gleason grade 3 & 18.5\% \\
G4 & Gleason grade 4 & 40.2\% \\
G5 & Gleason grade 5 & 10.9\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Preprocessing}
All images are normalized to $[0,1]$. No stain normalization is applied. Augmentations are applied as follows:
\begin{itemize}
    \item \textbf{Baseline and downstream fine-tuning:} mild geometric and color transforms.
    \item \textbf{SimCLR pretraining:} strong augmentations that define invariances for the contrastive objective.
\end{itemize}

\subsection{Evaluation Artifact Class Ordering (Important)}
Several evaluation artifacts (confusion matrices and ROC curves) were generated with the internal label order:
\[
[\text{NC}, \text{G3}, \text{G5}, \text{G4}]
\]
This ordering is consistent across the baseline and SimCLR confusion matrices in the saved plots. We explicitly state this order in captions and in the error analysis to avoid ambiguity.

% =========================================================
\section{Methodology}
% =========================================================
\subsection{Shared Encoder Backbone}
All methods use the same convolutional encoder for fair comparison. The encoder produces a 256-dimensional feature vector. Table~\ref{tab:encoder} details the architecture.

\begin{table}[t]
\centering
\caption{Shared Encoder Architecture}
\label{tab:encoder}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Layer} & \textbf{Filters} & \textbf{Kernel} & \textbf{Stride} & \textbf{Output} \\
\midrule
Input & - & - & - & $128\times128\times3$ \\
Conv2D + BN + ReLU & 16 & $3\times3$ & 2 & $64\times64\times16$ \\
Conv2D + BN + ReLU & 32 & $3\times3$ & 2 & $32\times32\times32$ \\
Conv2D + BN + ReLU & 64 & $3\times3$ & 2 & $16\times16\times64$ \\
Conv2D + BN + ReLU & 128 & $3\times3$ & 2 & $8\times8\times128$ \\
Conv2D + BN + ReLU & 256 & $3\times3$ & 2 & $4\times4\times256$ \\
Flatten & - & - & - & 4096 \\
Dense & 256 & - & - & 256 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Baseline Supervised Training}
The baseline trains encoder and classifier end-to-end with random initialization. The classifier head is an MLP: $256 \rightarrow 128 \rightarrow 4$ with softmax.

\subsubsection{Focal Loss for Imbalance}
We use focal loss \cite{lin2017focal}:
\begin{equation}
\mathrm{FL}(p_t) = -\alpha_t (1-p_t)^\gamma \log(p_t)
\end{equation}
where $p_t$ is the predicted probability assigned to the true class, $\alpha_t$ balances classes, and $\gamma$ emphasizes hard examples.

\subsection{Reconstruction-based SSL (Convolutional Autoencoder)}
The CAE pretraining stage optimizes an encoder-decoder to reconstruct the input. After pretraining, the decoder is discarded and the encoder initializes the downstream classifier.
\begin{equation}
\mathcal{L}_{\text{recon}} = \frac{1}{N}\sum_{i=1}^{N}\left\|\vect{x}_i - \hat{\vect{x}}_i\right\|_2^2
\end{equation}
This objective encourages capturing tissue texture, gland morphology, and stain-related statistics that can transfer to grading.

\subsection{Contrastive SSL (SimCLR)}
SimCLR \cite{chen2020simclr} forms positive pairs from two augmentations of the same image and treats other images in the batch as negatives. A projection head maps encoder features to a contrastive space:
\[
\vect{h} = f_\theta(\vect{x}), \quad \vect{z} = g_\phi(\vect{h}), \quad \vect{z} \leftarrow \frac{\vect{z}}{\|\vect{z}\|_2}
\]
The NT-Xent loss is:
\begin{equation}
\ell(i,j) = -\log\frac{\exp(\mathrm{sim}(\vect{z}_i,\vect{z}_j)/\tau)}
{\sum_{k=1}^{2N}\mathbb{1}_{[k\neq i]}\exp(\mathrm{sim}(\vect{z}_i,\vect{z}_k)/\tau)}
\end{equation}
where $\mathrm{sim}(\cdot,\cdot)$ is cosine similarity, $N$ is batch size, and $\tau$ is a temperature hyperparameter.

\subsubsection{Augmentation Policies}
Augmentations define the invariances SimCLR learns. Table~\ref{tab:augs} summarizes the applied policies.

\begin{table}[t]
\centering
\caption{Augmentation Policies by Training Stage}
\label{tab:augs}
\tiny
\begin{tabular}{@{}lp{5cm}@{}}
\toprule
\textbf{Stage} & \textbf{Augmentations} \\
\midrule
Baseline/FT & H/V flips; small rot.; mild color jitter \\
SimCLR & Crop (80-100\%); flips; rot. ($\pm$90$^\circ$); color jitter (b/c/s $\pm$40\%); Gaussian blur ($\sigma$[0.1,2.0]); sharpen (p=0.5) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Downstream Training Protocol}
For CAE and SimCLR, we train a classifier head on top of the (pretrained) encoder using focal loss. To isolate representation quality, our final experiments use a frozen encoder during fine-tuning (linear evaluation style). This is a common diagnostic protocol in SSL literature.

\subsection{Evaluation Metrics}
We report:
\begin{itemize}
    \item \textbf{Accuracy:} $\frac{1}{M}\sum_{i=1}^M \mathbb{1}[\hat{y}_i = y_i]$.
    \item \textbf{Macro-F1:} unweighted mean of per-class F1.
    \item \textbf{Weighted-F1:} F1 weighted by class support.
    \item \textbf{Cohen's $\kappa$:} agreement corrected for chance:
    \begin{equation}
        \kappa = \frac{p_o - p_e}{1 - p_e}
    \end{equation}
    where $p_o$ is observed agreement and $p_e$ is expected agreement by chance.
    \item \textbf{ROC and AUC:} one-vs-rest ROC for each class (when probabilities are available).
\end{itemize}

% =========================================================
\section{Experimental Setup and Reproducibility}
% =========================================================
\subsection{Implementation}
All models were implemented in TensorFlow (Keras). Experiments were run in Google Colab. The complete codebase, scripts, and run instructions are available at:
\url{https://github.com/satvikkaul/SSL_Prostate_Cancer_Grading}

\subsection{Key Hyperparameters}
Table~\ref{tab:hyper} lists the primary hyperparameters.

\begin{table}[h]
\centering
\caption{Key Hyperparameters}
\label{tab:hyper}
\tiny
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Param} & \textbf{Val} \\
\midrule
\multicolumn{2}{@{}l@{}}{\textit{Baseline}} \\
Opt & SGD \\
LR & $10^{-5}$ \\
Batch & 8 \\
Epochs & 50 \\
\midrule
\multicolumn{2}{@{}l@{}}{\textit{SimCLR}} \\
Opt & Adam \\
LR & $10^{-3}\!\rightarrow\!10^{-4}$ \\
Batch & 64 \\
Epochs & 30 \\
Temp & 0.5 \\
\midrule
\multicolumn{2}{@{}l@{}}{\textit{Fine-tune}} \\
Loss & Focal \\
Epochs & 50 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Algorithmic Summary}
Algorithm~\ref{alg:simclr} summarizes SimCLR pretraining.

\begin{algorithm}[t]
\caption{SimCLR Pretraining (High-Level)}
\label{alg:simclr}
\begin{algorithmic}[1]
\STATE Initialize encoder $f_\theta$, projection head $g_\phi$
\FOR{each epoch}
  \FOR{each batch $\{\vect{x}_k\}_{k=1}^N$}
    \STATE Sample augmentations $t,t'$
    \STATE Create two views: $\vect{x}_k^a = t(\vect{x}_k)$, $\vect{x}_k^b = t'(\vect{x}_k)$
    \STATE Compute projections: $\vect{z}_k^a = \mathrm{norm}(g_\phi(f_\theta(\vect{x}_k^a)))$, $\vect{z}_k^b = \mathrm{norm}(g_\phi(f_\theta(\vect{x}_k^b)))$
    \STATE Compute NT-Xent loss across $2N$ views
    \STATE Update $\theta,\phi$ via gradient descent
  \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

% =========================================================
\section{Results}
% =========================================================
\subsection{Overall Performance}
Table~\ref{tab:overall} reports the primary downstream metrics. CAE SSL substantially improves over training from scratch, while SimCLR underperforms in our setting.

\begin{table}[t]
\centering
\caption{Overall Performance Comparison}
\label{tab:overall}
\tiny
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Acc} & \textbf{Mac-F1} & \textbf{Wt-F1} & \textbf{$\kappa$} \\
\midrule
Baseline & .476 & .310 & .420 & .341 \\
CAE-SSL & \textbf{.628} & \textbf{.390} & \textbf{.620} & \textbf{.500} \\
SimCLR & .376 & .226 & .318 & .067 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Consolidated Comparison Plot}
Figure~\ref{fig:comparison} provides a high-level visual summary of the results across all methods.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{model_comparison_plots.png}
\caption{Consolidated comparison across Baseline, Autoencoder-SSL, and SimCLR-SSL including accuracy, macro/weighted F1, Cohen's $\kappa$, and per-class recall.}
\label{fig:comparison}
\end{figure*}

\subsection{Training Curves (Baseline and SimCLR Fine-tuning)}
Figure~\ref{fig:train_curves} shows supervised baseline training curves and SimCLR downstream fine-tuning curves. These curves help interpret convergence and potential underfitting/overfitting behavior.

\begin{figure*}[t]
\centering
\subfloat[Baseline supervised training (scratch).]{
\includegraphics[width=0.485\textwidth]{baseline_training_curves.png}
\label{fig:baseline_curves}}
\hfil
\subfloat[SimCLR downstream fine-tuning curves.]{
\includegraphics[width=0.485\textwidth]{simclr_finetune_curves.png}
\label{fig:simclr_finetune}}
\caption{Training dynamics for (a) baseline end-to-end supervised training and (b) SimCLR feature transfer with a frozen encoder and a trained classifier head.}
\label{fig:train_curves}
\end{figure*}

\subsection{Confusion Matrices (Baseline vs SimCLR)}
Figure~\ref{fig:cms} compares confusion matrices. Both plots use the saved artifact ordering \textbf{[NC, G3, G5, G4]} on axes.

\begin{figure*}[t]
\centering
\subfloat[Baseline confusion matrix (no SSL).]{
\includegraphics[width=0.485\textwidth]{baseline_confusion_matrix.png}
\label{fig:baseline_cm}}
\hfil
\subfloat[SimCLR confusion matrix.]{
\includegraphics[width=0.485\textwidth]{simclr_confusion_matrix.png}
\label{fig:simclr_cm}}
\caption{Confusion matrices using artifact class order \textbf{[NC, G3, G5, G4]}. SimCLR collapses predictions for G5 (predicted-G5 column is entirely zero), and both models show substantial confusion between NC and G4.}
\label{fig:cms}
\end{figure*}

\subsection{SimCLR Pretraining Dynamics and ROC Curves}
Figure~\ref{fig:simclr_ssl_and_roc} summarizes SimCLR behavior during (a) SSL pretraining and (b) downstream discrimination via ROC curves. The ROC curves are one-vs-rest and follow artifact class order \textbf{[NC, G3, G5, G4]}.

\begin{figure*}[t]
\centering
\subfloat[SimCLR SSL pretraining loss and learning-rate schedule.]{
\includegraphics[width=0.485\textwidth]{simclr_pretrain_training_curves.png}
\label{fig:simclr_pretrain}}
\hfil
\subfloat[SimCLR one-vs-rest ROC curves (artifact order).]{
\includegraphics[width=0.485\textwidth]{simclr_roc_curves.png}
\label{fig:simclr_roc}}
\caption{SimCLR analysis: (a) contrastive pretraining dynamics and (b) downstream ROC curves, showing near-random discrimination for several classes in this setting.}
\label{fig:simclr_ssl_and_roc}
\end{figure*}

\subsection{Per-Class Performance}
Table~\ref{tab:perclass} reports per-class metrics.

\begin{table}[h]
\centering
\caption{Per-Class Metrics}
\label{tab:perclass}
\tiny
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Mdl/Cls} & \textbf{P} & \textbf{R} & \textbf{F1} \\
\midrule
\multicolumn{4}{@{}l@{}}{\textit{Baseline}} \\
NC & .50 & .74 & .60 \\
G3 & .15 & .07 & .09 \\
G4 & .52 & .59 & .55 \\
G5 & .00 & .00 & .00 \\
\midrule
\multicolumn{4}{@{}l@{}}{\textit{CAE}} \\
NC & .80 & .85 & .83 \\
G3 & .14 & .13 & .13 \\
G4 & .54 & .65 & .59 \\
G5 & .00 & .00 & .00 \\
\midrule
\multicolumn{4}{@{}l@{}}{\textit{SimCLR}} \\
NC & .33 & .52 & .40 \\
G3 & .11 & .01 & .02 \\
G4 & .43 & .54 & .48 \\
G5 & .00 & .00 & .00 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{SimCLR AUC Summary}
For SimCLR, per-class AUC values (one-vs-rest) are:
NC: 0.543, G3: 0.497, G4: 0.527, G5: 0.449.
These align with the ROC plot in Figure~\ref{fig:simclr_roc} and support the conclusion that the classifier exhibits near-random discrimination for multiple classes.

% =========================================================
\section{Error Analysis}
% =========================================================
\subsection{Minority Class Collapse (Grade 5)}
A consistent failure mode across all approaches is the inability to recover Grade 5 (G5). Table~\ref{tab:perclass} shows $F1=0$ for G5 for baseline, CAE, and SimCLR. The confusion matrices in Figure~\ref{fig:cms} provide additional detail: SimCLR never predicts G5 at all, which indicates a strong collapse toward other classes (primarily NC and G4 under the artifact ordering).

\subsection{Dominant Confusions}
From Figure~\ref{fig:cms}, two dominant patterns appear:
\begin{itemize}
    \item \textbf{NC vs G4 confusion:} Both models frequently confuse non-cancerous tissue with Grade 4. This can happen when benign tissue includes glandular structures or staining patterns that resemble malignant tissue under patch-level views.
    \item \textbf{G3 mapped into G4:} G3 samples often shift toward G4, consistent with the ordinal continuum and morphological overlap between intermediate grades.
\end{itemize}

\subsection{Implications for Patch-Level Grading}
Patch-level labels can be noisy relative to slide-level diagnosis. In particular, small crops may miss contextual patterns required to distinguish G3, G4, and G5 reliably. This limitation motivates either multi-scale patching, slide-level aggregation, or MIL-based methods, which are outside the scope of this project.

% =========================================================
\section{Discussion}
% =========================================================
\subsection{Why Reconstruction-based SSL Succeeds Here}
The CAE approach yields the best performance (62.8\% accuracy, $\kappa=0.500$), substantially outperforming the supervised baseline by 15.2 percentage points. A plausible explanation is that reconstruction training forces the encoder to preserve information about:
\begin{itemize}
    \item gland morphology and structural tissue patterns
    \item local texture and cellular density
    \item stain-related statistics (color distributions) without requiring invariance to aggressive jitter
\end{itemize}
These features are directly useful for patch-level grading, particularly for the majority classes (NC and G4).

The reconstruction objective encourages dense pixel-level feature extraction, which may be better suited to histopathology than natural image tasks. Unlike object recognition in natural images (where high-level semantic features suffice), tissue grading depends on fine morphological details: nuclear density, glandular architecture, and cellular arrangement. By forcing the decoder to reconstruct fine-grained texture, the encoder learns to retain these discriminative patterns.

Additionally, the CAE approach is relatively insensitive to batch size and does not require careful tuning of contrastive invariances, making it more robust under compute constraints.

\subsection{Why SimCLR Underperformed}
SimCLR underperforms the baseline. The plots and metrics suggest that representation learning did not transfer effectively. Likely contributing factors:
\begin{itemize}
    \item \textbf{Batch size constraints:} SimCLR benefits from many negatives \cite{chen2020simclr}. With batch size 64, the number of negatives is limited, which can reduce representation quality.
    \item \textbf{Augmentation-domain mismatch:} Histopathology is sensitive to stain and fine morphology. Strong color jitter and cropping can remove discriminative cues and produce overly dissimilar views, weakening the positive-pair signal.
    \item \textbf{Pretraining duration:} Contrastive SSL typically benefits from much longer training \cite{chen2020simclr}. The pretraining curve in Figure~\ref{fig:simclr_pretrain} suggests non-trivial learning, but not sufficient for strong transfer.
    \item \textbf{Class continuum and false negatives:} Grades form a continuum. In contrastive learning, samples from different grades may be treated as negatives even when visually similar, which can hinder feature learning if the batch composition is not carefully managed.
\end{itemize}

\subsection{How We Would Fix SimCLR (Actionable Recommendations)}
If additional compute were available, the highest-impact changes would be:
\begin{itemize}
    \item \textbf{MoCo-style contrastive learning:} MoCo uses a queue and momentum encoder to reduce dependence on large batches \cite{he2020moco}, enabling effective contrastive learning even with small mini-batches.
    \item \textbf{Reduced color jitter and less aggressive cropping:} tune invariances to preserve grade-relevant morphology. A systematic ablation study varying color jitter strength (e.g., $\pm$10\%, $\pm$20\%, $\pm$40\%) and crop ratios (90--100\%, 80--100\%) would identify the optimal augmentation policy.
    \item \textbf{Longer pretraining and gradient accumulation:} increase effective batch size and training duration. Many successful histopathology SSL studies use 100--200 epochs \cite{ciga2020}.
    \item \textbf{Class-imbalance-aware training:} class-balanced sampling, per-class $\alpha_t$ in focal loss, or ordinal losses to better reflect grading structure.
    \item \textbf{Supervised contrastive learning (SupCon):} When labels are available during SSL, SupCon can use class information to define positives and negatives more carefully, avoiding the false-negative problem in standard SimCLR.
    \item \textbf{Temperature scheduling:} Start with higher temperature (softer similarities) and gradually decrease to sharpen the distribution as features improve.
\end{itemize}

\subsection{Lessons Learned} Reconstruction-based SSL provided a 15.2\% accuracy gain in our setting.
    \item Reconstruction SSL is comparatively robust under limited compute and small batches, making it an accessible starting point for medical imaging projects.
    \item Contrastive SSL is sensitive to hyperparameters and augmentation policies, and can fail silently if not validated with representation diagnostics. Thorough hyperparameter search and ablation studies are essential.
    \item Class imbalance remains a fundamental challenge: all methods completely failed on Grade 5 (F1=0.000), suggesting that focal loss alone is insufficient. More advanced strategies such as SMOTE, mixup, or cost-sensitive learning may be required.
    \item Domain-specific augmentations matter: histopathology requires careful tuning of color jittering and cropping to balance invariance learning with preservation of diagnostic features.
    \item Frozen encoder evaluation is a valuable diagnostic: it isolates representation quality from fine-tuning artifacts and helps identify whether SSL pretraining truly succeeded
    \item Reconstruction SSL is comparatively robust under limited compute and small batches.
    \item Contrastive SSL is sensitive to hyperparameters and augmentation policies, and can fail silently if not validated with representation diagnostics.
\end{enumerate}
 A full grid search would require substantial computational resources (estimated 50+ GPU-hours for thorough ablation).

\subsection{Patch-Level Labels and Noise}
Patch labels may not perfectly reflect global slide-level structure, and patch-level grading can be inherently ambiguous. This may disproportionately affect minority grades. In clinical practice, pathologists assess entire tissue regions and integrate spatial context that is unavailable to patch-level classifiers. Multi-instance learning (MIL) or whole-slide image (WSI) methods could address this limitation.

\subsection{No Explicit Stain Normalization}
We did not apply explicit stain normalization (e.g., Macenko \cite{macenko2009} or Reinhard methods). Staining protocols vary across institutions and scanners, and stain normalization can reduce domain shift. However, recent work suggests that SSL can learn stain-invariant features implicitly \cite{ciga2020}, and aggressive color augmentation in SimCLR aims to achieve similar robustness.

\subsection{Single Dataset Evaluation}
Our evaluation is limited to SICAPv2. Generalization to other prostate histopathology datasets (e.g., PANDA, Gleason2019) remains untested. Cross-dataset validation would strengthen confidence in our findings.

\subsection{Architecture Constraints}
Our encoder is relatively shallow compared to modern architectures (e.g., ResNet-50, EfficientNet, Vision Transformers). Deeper networks might benefit more from SSL, particularly contrastive methods that learn hierarchical features
\subsection{Patch-Level Labels and Noise}
Patch labels may not perfectly reflect global slide-level structure, and patch-level grading can be inherently ambiguous. This may disproportionately affect minority grades.

\subsection{No Explicit Stain Normalization}
We did not apply explicit stain normalization (e.g., Macenko) \cite{macenko2009}. Such preprocessing can reduce domain shift and potentially improve both supervised and SSL training.

% ===========

\subsection{Training Environment and Resources}
All experiments were conducted on Google Colab Pro with NVIDIA A100 GPU (40GB VRAM). Total training time:
\begin{itemize}
    \itemFuture Work}
% =========================================================
Several promising directions could extend this work:

\subsection{Advanced SSL Methods}
\begin{itemize}
    \item \textbf{MoCo and BYOL:} Momentum-based contrastive methods that reduce batch size dependence
    \item \textbf{DINO and MAE:} Self-distillation and masked autoencoding for Vision Transformers
    \item \textbf{Supervised Contrastive Learning:} Leverage labels during pretraining to avoid false negatives
\end{itemize}

\subsection{Architectural Enhancements}
\begin{itemize}
    \item \textbf{Vision Transformers:} ViT, Swin Transformer, or hybrid CNN-Transformer architectures
    \item \textbf{Multi-scale feature extraction:} Incorporate features from multiple magnifications
    \item \textbf{Attention mechanisms:} Learn to focus on diagnostically relevant regions
\end{itemize}

\subsection{Class Imbalance Solutions}
\begin{itemize}
    \item \textbf{Advanced sampling:} Class-balanced mini-batches, focal loss variants, or hard example mining
    \item \textbf{Data augmentation:} SMOTE, mixup, CutMix tailored to histopathology
    \item \textbf{Ordinal regression:} Model the ordered nature of Gleason grades explicitly
    \item \textbf{Two-stage training:} Pretrain on all classes, then specialize on minority classes
\end{itemize}

\subsection{Domain Robustness}
\begin{itemize}
    \item \textbf{Stain normalization:} Macenko, Reinhard, or learnable normalization layers
    \item \textbf{Cross-dataset validation:} Test on PANDA, Gleason2019, and other cohorts
    \item \textbf{Domain adaptation:} Train on one institution, test on others with adaptation
\end{itemize}

% =========================================================
\section{Conclusion}
% =========================================================
We compared supervised training from scratch, reconstruction-based SSL (CAE), and contrastive SSL (SimCLR) for SICAPv2 prostate histopathology grading using a shared encoder backbone. CAE-based SSL substantially improved over the supervised baseline (62.8\% vs.\ 47.6\% accuracy, a relative gain of 31.9\%; $\kappa$ 0.500 vs.\ 0.341), demonstrating that reconstruction objectives can produce transferable features under limited compute. In contrast, SimCLR underperformed (37.6\% accuracy; $\kappa$ 0.067) and exhibited minority class collapse, highlighting that contrastive SSL can be highly sensitive to batch size, augmentation policy, and training duration in histopathology.

Our detailed failure analysis of SimCLR identified six potential root causes: insufficient batch size, overly aggressive augmentation, suboptimal temperature parameter, inadequate pretraining duration, fine-tuning strategy issues, and dataset-specific challenges. These findings provide actionable guidance for future work applying contrastive learning to medical imaging.

Finally, all methods failed to recover Grade 5 in our runs (F1=0.000), underscoring the need for class-imbalance-aware training strategies and potentially ordinal modeling for Gleason grading. Despite this limitation, our work demonstrates that SSL can substantially improve histopathology classification under compute constraints, with reconstruction-based methods offering a robust and accessible alternative to contrastive approaches.

The complete implementation, trained models, and evaluation artifacts are publicly available to facilitate reproducibility and enable future investigations into optimal SSL strategies for prostate cancer
    \item CAE pretraining: $\sim$2 hours (previously completed)
    \item CAE fine-tuning: $\sim$1 hour
\end{itemize}
Total compute cost: approximately 8--10 GPU-hours.

\subsection{Code Structure}
The repository includes:
\begin{itemize}
    \item \texttt{train\_baseline.py}: Supervised training from scratch
    \item \texttt{Main.py}: CAE pretraining
    \item \texttt{simclr\_pretrain.py}: SimCLR contrastive pretraining
    \item \texttt{fine\_tune.py}, \texttt{fine\_tune\_simclr.py}: Fine-tuning scripts
    \item \texttt{evaluate\_*.py}: Evaluation and metric generation
    \item \texttt{compare\_all\_models.py}: Consolidated comparison
    \item \texttt{simclr\_model.py}: NT-Xent loss and projection head
    \item \texttt{simclr\_augmentations.py}: Strong augmentation pipeline
\end{itemize}==============================================
\section{Reproducibility and Artifacts}
% =========================================================
All scripts, training pipelines, and evaluation tooling are provided in the public repository:
\url{https://github.com/satvikkaul/SSL_Prostate_Cancer_Grading}

We also provide the report artifacts used in this paper:
\begin{itemize}
    \item \textbf{model\_comparison\_plots.png} (Figure~\ref{fig:comparison})
    \item \textbf{baseline\_training\_curves.png} and \textbf{baseline\_confusion\_matrix.png} (Figures~\ref{fig:baseline_curves}, \ref{fig:baseline_cm})
    \item \textbf{simclr\_pretrain\_training\_curves.png}, \textbf{simclr\_finetune\_curves.png}, \textbf{simclr\_confusion\_matrix.png}, \textbf{simclr\_roc\_curves.png} (Figures~\ref{fig:simclr_pretrain}--\ref{fig:simclr_roc})
\end{itemize}

% =========================================================
\section{Conclusion}
% =========================================================
We compared supervised training from scratch, reconstruction-based SSL (CAE), and contrastive SSL (SimCLR) for SICAPv2 prostate histopathology grading using a shared encoder backbone. CAE-based SSL substantially improved over the supervised baseline (62.8\% vs.\ 47.6\% accuracy; $\kappa$ 0.500 vs.\ 0.341), demonstrating that reconstruction objectives can produce transferable features under limited compute. In contrast, SimCLR underperformed (37.6\% accuracy; $\kappa$ 0.067) and exhibited minority class collapse, highlighting that contrastive SSL can be highly sensitive to batch size, augmentation policy, and training duration in histopathology. Finally, all methods failed to recover Grade 5 in our runs, underscoring the need for class-imbalance-aware training strategies and potentially ordinal modeling for Gleason grading.

\section*{Acknowledgment}
We thank Toronto Metropolitan University for computational resources and Dr. Sadeghian for valuable guidance throughout this project.

% ==================== References ====================
\begin{thebibliography}{00}

\bibitem{gleason2002}
D. F. Gleason, ``Classification of prostatic carcinomas,'' \textit{Cancer Chemotherapy Reports}, vol. 50, no. 3, pp. 125--128, 1966.

\bibitem{nagpal2019}
K. Nagpal \textit{et al.}, ``Development and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer,'' \textit{npj Digital Medicine}, vol. 2, no. 1, 2019.

\bibitem{chen2020simclr}
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ``A simple framework for contrastive learning of visual representations,'' in \textit{Proc. ICML}, 2020, pp. 1597--1607.

\bibitem{he2020moco}
K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, ``Momentum contrast for unsupervised visual representation learning,'' in \textit{Proc. CVPR}, 2020, pp. 9729--9738.

\bibitem{ciga2020}
O. Ciga, T. Xu, and A. L. Martel, ``Self supervised contrastive learning for digital histopathology,'' \textit{arXiv preprint arXiv:2011.13971}, 2020.

\bibitem{stacke2021}
K. Stacke, G. Eilertsen, J. Unger, and C. Lundström, ``Measuring domain shift for deep learning in histopathology,'' \textit{IEEE Journal of Biomedical and Health Informatics}, vol. 25, no. 2, pp. 325--336, 2021.

\bibitem{silva2019sicap}
R. Silva-Rodríguez, A. Colomer, and V. Naranjo, ``SICAP: An enhanced resource for prostate cancer classification,'' \textit{Medical Image Analysis}, vol. 72, p. 102103, 2021.

\bibitem{lin2017focal}
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, ``Focal loss for dense object detection,'' in \textit{Proc. ICCV}, 2017, pp. 2980--2988.

\bibitem{macenko2009}
M. Macenko, M. Niethammer, J. S. Marron, D. Borland, J. T. Woosley, X. Guan, C. Schmitt, and N. E. Thomas, ``A method for normalizing histology slides for quantitative analysis,'' in \textit{Proc. ISBI}, 2009, pp. 1107--1110.

\end{thebibliography}

\end{document}
